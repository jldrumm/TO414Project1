---
title: "Project1"
author: "Jasper Drumm, Leif Gullstad, Tommy Papesh, Aidan Hatzer, Jonah Cuenca"
date: "2022-10-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Cost Benefit Analysis

```{}
We will begin our report by starting with a cost-benefit analysis for the Telemarketing company. Because each call costs $1 for the company and each successful sale results in $6 of profit that means to at least break even 1 out of every 6 calls (16.67%) needs to be a successful sale. Therefore, our goal will be to create a prediction model which will result in at least 16.67% of the recommended calls being successful, thus ensuring the company breaks even, which is all they are concerned about according to knowledge given in class. 
```


## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```


## Starting With Clustering
```{r}
categories <- tele_norm #tele_norm is normalized using the normalize function we wrote 
categories$yyes <- NULL


categories_z <- as.data.frame(lapply(categories, scale)) # run on the normalized data so it is all numeric
set.seed(12345)
tele_clusters <- kmeans(categories_z, 5) 
tele_clusters$centers
tele_norm$cluster <- tele_clusters$cluster # put the cluster number with each tele record - leave clusters as numbers

aggregate(data = tele_norm, yyes ~ cluster, mean) # this gives the success rate
```

# Clustering Analysis

```{}
After doing the clustering analysis we have determined that no models should be created for clusters 3 and 4 because they have a success rate of 19.52% and 63.70% respectively. Based on the cost-benefit analysis done above both of these clusters would be profitable without any additional modeling work needing to be done, because as discussed above if we can get an accuracy of 16.67% or better than we will at least break even. 

Therefore we can now just make models for the 1st, 2nd, and 5th clusters to improve the accuracy when calling indidviduals in those clusters. 
```

# Create Cluster Variables
```{r}
cluster1_norm <- tele_norm[tele_norm$cluster == 1,]
cluster2_norm <- tele_norm[tele_norm$cluster == 2,]
cluster5_norm <- tele_norm[tele_norm$cluster == 5,]
str(cluster1_norm)
str(cluster2_norm)
str(cluster5_norm)
```

## Create the Majority Voting Scheme
```{r}
# Now create the majority voting data frame to be completed as we run the models later
cluster1_majority_vote <- data.frame(matrix(ncol = 5, nrow = (nrow(cluster1_norm) - floor(0.3 * nrow(cluster1_norm)))))
colnames(cluster1_majority_vote) <- c("actual", "ANN_predict", "KNN_predict", "LM_predict", "combined_predict")
cluster2_majority_vote <- data.frame(matrix(ncol = 5, nrow = (nrow(cluster2_norm) - floor(0.3 * nrow(cluster2_norm)))))
colnames(cluster2_majority_vote) <- c("actual", "ANN_predict", "KNN_predict", "LM_predict", "combined_predict")
cluster5_majority_vote <- data.frame(matrix(ncol = 5, nrow = (nrow(cluster5_norm) - floor(0.3 * nrow(cluster5_norm)))))
colnames(cluster5_majority_vote) <- c("actual", "ANN_predict", "KNN_predict", "LM_predict", "combined_predict")
```

> Now you are ready to build your ANN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.

## ANN function
```{r}

ANN_func <- function(cluster_norm) {
  # cluster_norm$cluster <- NULL
  set.seed(12345)
  cluster_sample <- sample(1:nrow(cluster_norm), floor(0.3 * nrow(cluster_norm))) # 30% size of cluster_norm to use for sample
  cluster_test <- cluster_norm[-cluster_sample,] # 70% for test
  cluster_train <- cluster_norm[cluster_sample,]
  
  library(caret)
  library(neuralnet)
  
  func_model <- neuralnet(formula = yyes~., data = cluster_train, hidden = 2)
  
  func_prediction <- predict(func_model, newdata = cluster_test, type = "response")
  func_prediction <- ifelse(func_prediction < 0.5, 0, 1)
  confusionMatrix(as.factor(cluster_test$yyes), as.factor(func_prediction))
  
  # return the func_prediction
  return(as.factor(func_prediction))
}
```
## Logistics Model Function
```{r}
LM_func <- function(cluster_norm) {
  library(gmodels)
  set.seed(12345)
  cluster_sample <- sample(1:nrow(cluster_norm), floor(0.3 * nrow(cluster_norm))) # 30% size of cluster_norm to use for sample
  cluster_test <- cluster_norm[-cluster_sample,] # 70% for test
  cluster_train <- cluster_norm[cluster_sample,]
  
  cluster_model <- glm(yyes ~ ., data = cluster_train, family = "binomial")
  
  glm_Prediction <- predict(cluster_model, newdata = cluster_test, type = "response")
  glm_Prediction <- ifelse(glm_Prediction < 0.5, 0, 1)
  summary(glm_Prediction)

  CrossTable(x = cluster_test$yyes, y = glm_Prediction, prop.chisq = F)
  confusionMatrix(as.factor(cluster_test$yyes), as.factor(glm_Prediction))
  return(as.factor(glm_Prediction))
}
```
## KNN Model Function
```{r}
KNN_func <- function(cluster_norm){

library(class)
library(caret)
library(gmodels)
set.seed(12345)

# just renamed indices variable
test_indices <- sample(1:nrow(cluster_norm), floor(0.3 * nrow(cluster_norm)))
cluster_test <- cluster_norm[-test_indices,] # 70% for test
cluster_train <- cluster_norm[test_indices,]

#labels must be the same length so get them from cluster1_norm
cluster_test_labels <- cluster_norm[-test_indices, "yyes"]
cluster_train_labels <- cluster_norm[test_indices, "yyes"]


#this cluster as a hwhole tends to have yyes = 0, so I set the k-val arbitrarily to a low number to be able to predict 1's - From Jake in OH
k_val <- 1

cluster_train$yyes <- NULL
cluster_test$yyes <- NULL

cluster_test_pred <- knn(train = cluster_train, test = cluster_test, cl = cluster_train_labels, k = k_val)

CrossTable(x = cluster_test_labels, y = cluster_test_pred, prop.chisq = FALSE)
confusionMatrix(as.factor(cluster_test_labels), as.factor(cluster_test_pred))
return(as.factor(cluster_test_pred))
}
```
# Function for setting test values actual for majority voting
```{r}
set_majority_actual <- function(cluster_norm){
  set.seed(12345)
  cluster_sample <- sample(1:nrow(cluster_norm), floor(0.3 * nrow(cluster_norm))) # 30% size of cluster_norm to use for sample
  cluster_test <- cluster_norm[-cluster_sample,] # 70% for test
  return(as.factor(cluster_test$yyes))
}
```


## Compare ANN, LM, KNN, and Majority Voting on Cluster 1
```{r, cache=T}
cluster1_majority_vote$ANN_predict <- ANN_func(cluster_norm = cluster1_norm)
cluster1_majority_vote$LM_predict <- LM_func(cluster_norm = cluster1_norm)
cluster1_majority_vote$KNN_predict <- KNN_func(cluster_norm = cluster1_norm)
cluster1_majority_vote$actual <- set_majority_actual(cluster_norm = cluster1_norm)
cluster1_majority_vote$combined_predict <- as.factor(ifelse(cluster1_majority_vote$ANN_predict == 1 && cluster1_majority_vote$LM_predict == 1, 1, ifelse(cluster1_majority_vote$ANN_predict == 1 && cluster1_majority_vote$KNN_predict == 1, 1, ifelse(cluster1_majority_vote$LM_predict == 1 && cluster1_majority_vote$KNN_predict == 1, 1, 0))))
```
## Compare ANN, LM, KNN, and Majority Voting on Cluster 2
```{r, cache=T}
cluster2_majority_vote$ANN_predict <- ANN_func(cluster_norm = cluster2_norm)
cluster2_majority_vote$LM_predict <- LM_func(cluster_norm = cluster2_norm)
# summary(cluster2_majority_vote$LM_predict) - this looks right but for some reason in the environment it shows way more 1's instead of 0's...
cluster2_majority_vote$LM_predict
cluster2_majority_vote$KNN_predict <- KNN_func(cluster_norm = cluster2_norm)
cluster2_majority_vote$actual <- set_majority_actual(cluster_norm = cluster2_norm)
cluster2_majority_vote$combined_predict <- as.factor(ifelse(cluster2_majority_vote$ANN_predict == 1 && cluster2_majority_vote$LM_predict == 1, 1, ifelse(cluster2_majority_vote$ANN_predict == 1 && cluster2_majority_vote$KNN_predict == 1, 1, ifelse(cluster2_majority_vote$LM_predict == 1 && cluster2_majority_vote$KNN_predict == 1, 1, 0))))
# print the majority scheme accuracy
#nrow(cluster2_majority_vote[cluster2_majority_vote$actual == cluster2_majority_vote$combined_predict]) / nrow(cluster2_majority_vote)
```

## Compare ANN, LM, KNN, and Majority Voting on Cluster 5
```{r, cache=T}
cluster5_majority_vote$ANN_predict <- ANN_func(cluster_norm = cluster5_norm)
cluster5_majority_vote$LM_predict <- LM_func(cluster_norm = cluster5_norm)
cluster5_majority_vote$KNN_predict <- KNN_func(cluster_norm = cluster5_norm)
cluster5_majority_vote$actual <- set_majority_actual(cluster_norm = cluster5_norm)
cluster5_majority_vote$combined_predict <- as.factor(ifelse(cluster5_majority_vote$ANN_predict == 1 && cluster5_majority_vote$LM_predict == 1, 1, ifelse(cluster5_majority_vote$ANN_predict == 1 && cluster5_majority_vote$KNN_predict == 1, 1, ifelse(cluster5_majority_vote$LM_predict == 1 && cluster5_majority_vote$KNN_predict == 1, 1, 0))))
```
## LM and ANN Analysis
```{}
Without KNN we cannot do a majority voting scheme, so we need to do our analysis purely based off of ANN and LM models for the chosen clusters.


```

